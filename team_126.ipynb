{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j01aH0PR4Sg-"
      },
      "source": [
        "# Project Draft\n",
        "Monisha Vatikuti\n",
        "\n",
        "mav12@illinois.edu\n",
        "\n",
        "Team 126\n",
        "\n",
        "Paper 16 - Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs\n",
        "\n",
        "Github Repo: https://github.com/mvattiku/cs598-dlh-project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "    * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "    * what is the importance/meaning of solving the problem\n",
        "    * what is the difficulty of the problem\n",
        "    * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "    * what did the paper propose\n",
        "    * what is the innovations of the method\n",
        "    * how well the proposed method work (in its own metrics)\n",
        "    * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we are trying to reproduce the new model called Chet that is proposed in the Context-aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs paper. The paper is trying to address the issue of viewing histocial diagnosis from EHR data as independent events which disregards potential relationships of diseases among visits. Most deep learning models for disease classification using longitudinal EHR data treat disease diagnoses as independent events within individual visits. But there exist patters in the co-occurrence of disease diagnoses that could be valuable for predicting future patient outcomes. And these patterns are being ignored in current set of models. So the paper proposes a novel deep learning model called Chet (context-aware health event prediction via transition functions on dynamic disease graphs) which looks at both the evolution of diseases and the relationships between diseases to predict future diagnoses. Chet aims to look for interlinked patterns by trying to learn how diseases are progressing over consecutive visits to anticipate future diagnoses.\n",
        "\n",
        "In particular, Chet model learns the evolution of diagnosed diseases across a patient's doctor visits and exploits this learned disease context to forecast future outcomes and diagnoses. The key innovation of this approach lies in its incorporation of both disease co-occurrence information and the dynamic nature of diseases into the model. To accomplish this, the model constructs a weighted disease combination based on the entire longitudinal EHR data globally, as well as a disease subgraph based on the specific visit locally. To account for the dynamic nature of diseases, the paper employs disease-level temporal learning with multiple diagnosis roles and corresponding transition functions to capture historical contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "In this replication study, I will adopt the same methodology proposed in the paper for data selection, cleaning, and preprocessing. I will be using the MIMIC-III and MIMIC-IV datasets and randomly divide the data into training, validation, and test sets as done in the original study. Then I will build the diagnosis graphs and compute the adjacency matrices for their corresponding subgraphs using the same steps as outlined in the paper. Then I will train the model for both diagnosis prediction and heart failure prediction and compare the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: Currently I am using **MIMIC-III Clinical Database Demo**. I download this data from https://physionet.org/content/mimiciii-demo/1.4/ and is located in this repo in the following directory `data/mimic3/raw`. But for the final project I will pull the full dataset. \n",
        "\n",
        "  * Data process: I am processing the data the same way as proposed in the paper. I have pulled the original scripts from papers repo (located under `Chet/` directory). And I am running the `run_preprocess.py` script on the dataset.\n",
        "\n",
        "  * Statistics: The output of the followin script prints out the necessary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function is copied from the original code (run_prepocess.py script)\n",
        "# This function also uses some scripts from preprocess module which has also been copied from the source code\n",
        "import os\n",
        "from sys import exit\n",
        "import _pickle as pickle\n",
        "\n",
        "from preprocess import save_sparse, save_data\n",
        "from preprocess.parse_csv import Mimic3Parser, Mimic4Parser, EICUParser\n",
        "from preprocess.encode import encode_code\n",
        "from preprocess.build_dataset import split_patients, build_code_xy, build_heart_failure_y\n",
        "from preprocess.auxiliary import generate_code_code_adjacent, generate_neighbors, normalize_adj, divide_middle, generate_code_levels\n",
        "\n",
        "def run_preprocess_modified(dataset='mimic3', from_saved=True):\n",
        "    conf = {\n",
        "        'mimic3': {\n",
        "            'parser': Mimic3Parser,\n",
        "            'train_num': 6000,\n",
        "            'test_num': 1000,\n",
        "            'threshold': 0.01\n",
        "        },\n",
        "        'mimic4': {\n",
        "            'parser': Mimic4Parser,\n",
        "            'train_num': 8000,\n",
        "            'test_num': 1000,\n",
        "            'threshold': 0.01,\n",
        "            'sample_num': 10000\n",
        "        },\n",
        "        'eicu': {\n",
        "            'parser': EICUParser,\n",
        "            'train_num': 8000,\n",
        "            'test_num': 1000,\n",
        "            'threshold': 0.01\n",
        "        }\n",
        "    }\n",
        "    # from_saved = True\n",
        "    data_path = 'data'\n",
        "    # dataset = 'mimic3'  # mimic3, eicu, or mimic4\n",
        "    dataset_path = os.path.join(data_path, dataset)\n",
        "    raw_path = os.path.join(dataset_path, 'raw')\n",
        "    # if not os.path.exists(raw_path):\n",
        "    #     os.makedirs(raw_path)\n",
        "    #     print('please put the CSV files in `data/%s/raw`' % dataset)\n",
        "    #     exit()\n",
        "    parsed_path = os.path.join(dataset_path, 'parsed')\n",
        "    if from_saved:\n",
        "        patient_admission = pickle.load(open(os.path.join(parsed_path, 'patient_admission.pkl'), 'rb'))\n",
        "        admission_codes = pickle.load(open(os.path.join(parsed_path, 'admission_codes.pkl'), 'rb'))\n",
        "    else:\n",
        "        parser = conf[dataset]['parser'](raw_path)\n",
        "        sample_num = conf[dataset].get('sample_num', None)\n",
        "        patient_admission, admission_codes = parser.parse(sample_num)\n",
        "        print('saving parsed data ...')\n",
        "        if not os.path.exists(parsed_path):\n",
        "            os.makedirs(parsed_path)\n",
        "        pickle.dump(patient_admission, open(os.path.join(parsed_path, 'patient_admission.pkl'), 'wb'))\n",
        "        pickle.dump(admission_codes, open(os.path.join(parsed_path, 'admission_codes.pkl'), 'wb'))\n",
        "\n",
        "    patient_num = len(patient_admission)\n",
        "    max_admission_num = max([len(admissions) for admissions in patient_admission.values()])\n",
        "    avg_admission_num = sum([len(admissions) for admissions in patient_admission.values()]) / patient_num\n",
        "    max_visit_code_num = max([len(codes) for codes in admission_codes.values()])\n",
        "    avg_visit_code_num = sum([len(codes) for codes in admission_codes.values()]) / len(admission_codes)\n",
        "    print('patient num: %d' % patient_num)\n",
        "    print('max admission num: %d' % max_admission_num)\n",
        "    print('mean admission num: %.2f' % avg_admission_num)\n",
        "    print('max code num in an admission: %d' % max_visit_code_num)\n",
        "    print('mean code num in an admission: %.2f' % avg_visit_code_num)\n",
        "\n",
        "    print('encoding code ...')\n",
        "    admission_codes_encoded, code_map = encode_code(patient_admission, admission_codes)\n",
        "    code_num = len(code_map)\n",
        "    print('There are %d codes' % code_num)\n",
        "\n",
        "    code_levels = generate_code_levels(data_path, code_map)\n",
        "    pickle.dump({\n",
        "        'code_levels': code_levels,\n",
        "    }, open(os.path.join(parsed_path, 'code_levels.pkl'), 'wb'))\n",
        "\n",
        "    train_pids, valid_pids, test_pids = split_patients(\n",
        "        patient_admission=patient_admission,\n",
        "        admission_codes=admission_codes,\n",
        "        code_map=code_map,\n",
        "        train_num=conf[dataset]['train_num'],\n",
        "        test_num=conf[dataset]['test_num']\n",
        "    )\n",
        "    print('There are %d train, %d valid, %d test samples' % (len(train_pids), len(valid_pids), len(test_pids)))\n",
        "    code_adj = generate_code_code_adjacent(pids=train_pids, patient_admission=patient_admission,\n",
        "                                           admission_codes_encoded=admission_codes_encoded,\n",
        "                                           code_num=code_num, threshold=conf[dataset]['threshold'])\n",
        "\n",
        "    common_args = [patient_admission, admission_codes_encoded, max_admission_num, code_num]\n",
        "    print('building train codes features and labels ...')\n",
        "    (train_code_x, train_codes_y, train_visit_lens) = build_code_xy(train_pids, *common_args)\n",
        "    print('building valid codes features and labels ...')\n",
        "    (valid_code_x, valid_codes_y, valid_visit_lens) = build_code_xy(valid_pids, *common_args)\n",
        "    print('building test codes features and labels ...')\n",
        "    (test_code_x, test_codes_y, test_visit_lens) = build_code_xy(test_pids, *common_args)\n",
        "\n",
        "    print('generating train neighbors ...')\n",
        "    train_neighbors = generate_neighbors(train_code_x, train_visit_lens, code_adj)\n",
        "    print('generating valid neighbors ...')\n",
        "    valid_neighbors = generate_neighbors(valid_code_x, valid_visit_lens, code_adj)\n",
        "    print('generating test neighbors ...')\n",
        "    test_neighbors = generate_neighbors(test_code_x, test_visit_lens, code_adj)\n",
        "\n",
        "    print('generating train middles ...')\n",
        "    train_divided = divide_middle(train_code_x, train_neighbors, train_visit_lens)\n",
        "    print('generating valid middles ...')\n",
        "    valid_divided = divide_middle(valid_code_x, valid_neighbors, valid_visit_lens)\n",
        "    print('generating test middles ...')\n",
        "    test_divided = divide_middle(test_code_x, test_neighbors, test_visit_lens)\n",
        "\n",
        "    print('building train heart failure labels ...')\n",
        "    train_hf_y = build_heart_failure_y('428', train_codes_y, code_map)\n",
        "    print('building valid heart failure labels ...')\n",
        "    valid_hf_y = build_heart_failure_y('428', valid_codes_y, code_map)\n",
        "    print('building test heart failure labels ...')\n",
        "    test_hf_y = build_heart_failure_y('428', test_codes_y, code_map)\n",
        "\n",
        "    encoded_path = os.path.join(dataset_path, 'encoded')\n",
        "    if not os.path.exists(encoded_path):\n",
        "        os.makedirs(encoded_path)\n",
        "    print('saving encoded data ...')\n",
        "    pickle.dump(patient_admission, open(os.path.join(encoded_path, 'patient_admission.pkl'), 'wb'))\n",
        "    pickle.dump(admission_codes_encoded, open(os.path.join(encoded_path, 'codes_encoded.pkl'), 'wb'))\n",
        "    pickle.dump(code_map, open(os.path.join(encoded_path, 'code_map.pkl'), 'wb'))\n",
        "    pickle.dump({\n",
        "        'train_pids': train_pids,\n",
        "        'valid_pids': valid_pids,\n",
        "        'test_pids': test_pids\n",
        "    }, open(os.path.join(encoded_path, 'pids.pkl'), 'wb'))\n",
        "\n",
        "    print('saving standard data ...')\n",
        "    standard_path = os.path.join(dataset_path, 'standard')\n",
        "    train_path = os.path.join(standard_path, 'train')\n",
        "    valid_path = os.path.join(standard_path, 'valid')\n",
        "    test_path = os.path.join(standard_path, 'test')\n",
        "    if not os.path.exists(standard_path):\n",
        "        os.makedirs(standard_path)\n",
        "    if not os.path.exists(train_path):\n",
        "        os.makedirs(train_path)\n",
        "        os.makedirs(valid_path)\n",
        "        os.makedirs(test_path)\n",
        "\n",
        "    print('\\tsaving training data')\n",
        "    save_data(train_path, train_code_x, train_visit_lens, train_codes_y, train_hf_y, train_divided, train_neighbors)\n",
        "    print('\\tsaving valid data')\n",
        "    save_data(valid_path, valid_code_x, valid_visit_lens, valid_codes_y, valid_hf_y, valid_divided, valid_neighbors)\n",
        "    print('\\tsaving test data')\n",
        "    save_data(test_path, test_code_x, test_visit_lens, test_codes_y, test_hf_y, test_divided, test_neighbors)\n",
        "\n",
        "    code_adj = normalize_adj(code_adj)\n",
        "    save_sparse(os.path.join(standard_path, 'code_adj'), code_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "patient num: 7493\n",
            "max admission num: 42\n",
            "mean admission num: 2.66\n",
            "max code num in an admission: 39\n",
            "mean code num in an admission: 13.06\n",
            "encoding code ...\n",
            "There are 4880 codes\n",
            "generating code levels ...\n",
            "\t100%00%\n",
            "There are 6000 train, 493 valid, 1000 test samples\n",
            "generating code code adjacent matrix ...\n",
            "\t6000 / 6000\n",
            "building train codes features and labels ...\n",
            "\t6000 / 6000\n",
            "building valid codes features and labels ...\n",
            "\t493 / 493\n",
            "building test codes features and labels ...\n",
            "\t1000 / 1000\n",
            "generating train neighbors ...\n",
            "\t6000 / 6000\n",
            "generating valid neighbors ...\n",
            "\t493 / 493\n",
            "generating test neighbors ...\n",
            "\t1000 / 1000\n",
            "generating train middles ...\n",
            "\t6000 / 6000\n",
            "generating valid middles ...\n",
            "\t493 / 493\n",
            "generating test middles ...\n",
            "\t1000 / 1000\n",
            "building train heart failure labels ...\n",
            "building valid heart failure labels ...\n",
            "building test heart failure labels ...\n",
            "saving encoded data ...\n",
            "saving standard data ...\n",
            "\tsaving training data\n",
            "\tsaving valid data\n",
            "\tsaving test data\n"
          ]
        }
      ],
      "source": [
        "run_preprocess_modified(dataset='mimic3', from_saved=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "The model classes and functions have been pulled from the papers original code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from torch import nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Graph Layer\n",
        "This is the Optimized dynamic graph layer which extracts both local and global contexts for diagnoses and neighbors in visit t and then calculate hidden embeddings for diagnoses and neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphLayer(nn.Module):\n",
        "    def __init__(self, adj, code_size, graph_size):\n",
        "        super().__init__()\n",
        "        self.adj = adj\n",
        "        self.dense = nn.Linear(code_size, graph_size)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, code_x, neighbor, c_embeddings, n_embeddings):\n",
        "        center_codes = torch.unsqueeze(code_x, dim=-1)\n",
        "        neighbor_codes = torch.unsqueeze(neighbor, dim=-1)\n",
        "\n",
        "        center_embeddings = center_codes * c_embeddings\n",
        "        neighbor_embeddings = neighbor_codes * n_embeddings\n",
        "        cc_embeddings = center_codes * torch.matmul(self.adj, center_embeddings)\n",
        "        cn_embeddings = center_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
        "        nn_embeddings = neighbor_codes * torch.matmul(self.adj, neighbor_embeddings)\n",
        "        nc_embeddings = neighbor_codes * torch.matmul(self.adj, center_embeddings)\n",
        "\n",
        "        co_embeddings = self.activation(self.dense(center_embeddings + cc_embeddings + cn_embeddings))\n",
        "        no_embeddings = self.activation(self.dense(neighbor_embeddings + nn_embeddings + nc_embeddings))\n",
        "        return co_embeddings, no_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Transition Layer \n",
        "This is the transition functions layer which takes the hidden embeddings from the graph layer as inputs to this layer and applies GRU, M-GRU, or attention functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, query_size, key_size, value_size, attention_size):\n",
        "        super().__init__()\n",
        "        self.attention_size = attention_size\n",
        "        self.dense_q = nn.Linear(query_size, attention_size)\n",
        "        self.dense_k = nn.Linear(key_size, attention_size)\n",
        "        self.dense_v = nn.Linear(query_size, value_size)\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        query = self.dense_q(q)\n",
        "        key = self.dense_k(k)\n",
        "        value = self.dense_v(v)\n",
        "        g = torch.div(torch.matmul(query, key.T), math.sqrt(self.attention_size))\n",
        "        score = torch.softmax(g, dim=-1)\n",
        "        output = torch.sum(torch.unsqueeze(score, dim=-1) * value, dim=-2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, code_num, graph_size, hidden_size, t_attention_size, t_output_size):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRUCell(input_size=graph_size, hidden_size=hidden_size)\n",
        "        self.single_head_attention = SingleHeadAttentionLayer(graph_size, graph_size, t_output_size, t_attention_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        self.code_num = code_num\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, t, co_embeddings, divided, no_embeddings, unrelated_embeddings, hidden_state=None):\n",
        "        m1, m2, m3 = divided[:, 0], divided[:, 1], divided[:, 2]\n",
        "        m1_index = torch.where(m1 > 0)[0]\n",
        "        m2_index = torch.where(m2 > 0)[0]\n",
        "        m3_index = torch.where(m3 > 0)[0]\n",
        "        h_new = torch.zeros((self.code_num, self.hidden_size), dtype=co_embeddings.dtype).to(co_embeddings.device)\n",
        "        output_m1 = 0\n",
        "        output_m23 = 0\n",
        "        if len(m1_index) > 0:\n",
        "            m1_embedding = co_embeddings[m1_index]\n",
        "            h = hidden_state[m1_index] if hidden_state is not None else None\n",
        "            h_m1 = self.gru(m1_embedding, h)\n",
        "            h_new[m1_index] = h_m1\n",
        "            output_m1, _ = torch.max(h_m1, dim=-2)\n",
        "        if t > 0 and len(m2_index) + len(m3_index) > 0:\n",
        "            q = torch.vstack([no_embeddings[m2_index], unrelated_embeddings[m3_index]])\n",
        "            v = torch.vstack([co_embeddings[m2_index], co_embeddings[m3_index]])\n",
        "            h_m23 = self.activation(self.single_head_attention(q, q, v))\n",
        "            h_new[m2_index] = h_m23[:len(m2_index)]\n",
        "            h_new[m3_index] = h_m23[len(m2_index):]\n",
        "            output_m23, _ = torch.max(h_m23, dim=-2)\n",
        "        if len(m1_index) == 0:\n",
        "            output = output_m23\n",
        "        elif len(m2_index) + len(m3_index) == 0:\n",
        "            output = output_m1\n",
        "        else:\n",
        "            output, _ = torch.max(torch.vstack([output_m1, output_m23]), dim=-2)\n",
        "        return output, h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Embedding Layer\n",
        "This is the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, code_num, code_size, graph_size):\n",
        "        super().__init__()\n",
        "        self.code_num = code_num\n",
        "        self.c_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
        "        self.n_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, code_size)))\n",
        "        self.u_embeddings = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(code_num, graph_size)))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.c_embeddings, self.n_embeddings, self.u_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Classifier and Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, value_size, attention_size):\n",
        "        super().__init__()\n",
        "        self.attention_size = attention_size\n",
        "        self.context = nn.Parameter(data=nn.init.xavier_uniform_(torch.empty(attention_size, 1)))\n",
        "        self.dense = nn.Linear(value_size, attention_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = self.dense(x)\n",
        "        vu = torch.matmul(t, self.context).squeeze()\n",
        "        score = torch.softmax(vu, dim=-1)\n",
        "        output = torch.sum(x * torch.unsqueeze(score, dim=-1), dim=-2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dropout_rate=0., activation=None):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "        self.activation = activation\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.dropout(x)\n",
        "        output = self.linear(output)\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, code_num, code_size,\n",
        "                 adj, graph_size, hidden_size, t_attention_size, t_output_size,\n",
        "                 output_size, dropout_rate, activation):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = EmbeddingLayer(code_num, code_size, graph_size)\n",
        "        self.graph_layer = GraphLayer(adj, code_size, graph_size)\n",
        "        self.transition_layer = TransitionLayer(code_num, graph_size, hidden_size, t_attention_size, t_output_size)\n",
        "        self.attention = DotProductAttention(hidden_size, 32)\n",
        "        self.classifier = Classifier(hidden_size, output_size, dropout_rate, activation)\n",
        "\n",
        "    def forward(self, code_x, divided, neighbors, lens):\n",
        "        embeddings = self.embedding_layer()\n",
        "        c_embeddings, n_embeddings, u_embeddings = embeddings\n",
        "        output = []\n",
        "        for code_x_i, divided_i, neighbor_i, len_i in zip(code_x, divided, neighbors, lens):\n",
        "            no_embeddings_i_prev = None\n",
        "            output_i = []\n",
        "            h_t = None\n",
        "            for t, (c_it, d_it, n_it, len_it) in enumerate(zip(code_x_i, divided_i, neighbor_i, range(len_i))):\n",
        "                co_embeddings, no_embeddings = self.graph_layer(c_it, n_it, c_embeddings, n_embeddings)\n",
        "                output_it, h_t = self.transition_layer(t, co_embeddings, d_it, no_embeddings_i_prev, u_embeddings, h_t)\n",
        "                no_embeddings_i_prev = no_embeddings\n",
        "                output_i.append(output_it)\n",
        "            output_i = self.attention(torch.vstack(output_i))\n",
        "            output.append(output_i)\n",
        "        output = torch.vstack(output)\n",
        "        output = self.classifier(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import load_adj, EHRDataset, format_time, MultiStepLRScheduler\n",
        "from metrics import evaluate_codes, evaluate_hf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pulled these hyperparameters from source code (train.py)\n",
        "code_size = 48\n",
        "graph_size = 32\n",
        "hidden_size = 150  # rnn hidden size\n",
        "t_attention_size = 32\n",
        "t_output_size = hidden_size\n",
        "batch_size = 32\n",
        "epochs = 1  # 200  # decreased this to just 10 for testing purposes\n",
        "\n",
        "seed = 6669\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "use_cuda = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading train data ...\n",
            "loading valid data ...\n",
            "loading test data ...\n"
          ]
        }
      ],
      "source": [
        "# Loading the data\n",
        "dataset = 'mimic3'\n",
        "task = 'h'\n",
        "\n",
        "dataset_path = os.path.join('data', dataset, 'standard')\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "valid_path = os.path.join(dataset_path, 'valid')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "code_adj = load_adj(dataset_path, device=device)\n",
        "code_num = len(code_adj)\n",
        "print('loading train data ...')\n",
        "train_data = EHRDataset(train_path, label=task, batch_size=batch_size, shuffle=True, device=device)\n",
        "print('loading valid data ...')\n",
        "valid_data = EHRDataset(valid_path, label=task, batch_size=batch_size, shuffle=False, device=device)\n",
        "print('loading test data ...')\n",
        "test_data = EHRDataset(test_path, label=task, batch_size=batch_size, shuffle=False, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def historical_hot(code_x, code_num, lens):\n",
        "    result = np.zeros((len(code_x), code_num), dtype=int)\n",
        "    for i, (x, l) in enumerate(zip(code_x, lens)):\n",
        "        result[i] = x[l - 1]\n",
        "    return result\n",
        "    \n",
        "task_conf = {\n",
        "    'm': {\n",
        "        'dropout': 0.45,\n",
        "        'output_size': code_num,\n",
        "        'evaluate_fn': evaluate_codes,\n",
        "        'lr': {\n",
        "            'init_lr': 0.01,\n",
        "            'milestones': [20, 30],\n",
        "            'lrs': [1e-3, 1e-5]\n",
        "        }\n",
        "    },\n",
        "    'h': {\n",
        "        'dropout': 0.0,\n",
        "        'output_size': 1,\n",
        "        'evaluate_fn': evaluate_hf,\n",
        "        'lr': {\n",
        "            'init_lr': 0.01,\n",
        "            'milestones': [1, 2],\n",
        "            'lrs': [1e-3, 1e-4, 1e-5]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "721085\n",
            "Epoch 1 / 1:\n",
            "    Step 188 / 188, time cost: 5m11.6s, loss: 0.516161484\n",
            "    Evaluation: loss: 0.4826 --- auc: 0.8402 --- f1_score: 0.7253\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "test_historical = historical_hot(valid_data.code_x, code_num, valid_data.visit_lens)\n",
        "\n",
        "output_size = task_conf[task]['output_size']\n",
        "activation = torch.nn.Sigmoid()\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "evaluate_fn = task_conf[task]['evaluate_fn']\n",
        "dropout_rate = task_conf[task]['dropout']\n",
        "\n",
        "param_path = os.path.join('data', 'params', dataset, task)\n",
        "if not os.path.exists(param_path):\n",
        "    os.makedirs(param_path)\n",
        "\n",
        "model = Model(code_num=code_num, code_size=code_size,\n",
        "                adj=code_adj, graph_size=graph_size, hidden_size=hidden_size, t_attention_size=t_attention_size,\n",
        "                t_output_size=t_output_size,\n",
        "                output_size=output_size, dropout_rate=dropout_rate, activation=activation).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = MultiStepLRScheduler(optimizer, epochs, task_conf[task]['lr']['init_lr'],\n",
        "                                    task_conf[task]['lr']['milestones'], task_conf[task]['lr']['lrs'])\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch %d / %d:' % (epoch + 1, epochs))\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_num = 0\n",
        "    steps = len(train_data)\n",
        "    st = time.time()\n",
        "    scheduler.step()\n",
        "    for step in range(len(train_data)):\n",
        "        optimizer.zero_grad()\n",
        "        code_x, visit_lens, divided, y, neighbors = train_data[step]\n",
        "        output = model(code_x, divided, neighbors, visit_lens).squeeze()\n",
        "        loss = loss_fn(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * output_size * len(code_x)\n",
        "        total_num += len(code_x)\n",
        "\n",
        "        end_time = time.time()\n",
        "        remaining_time = format_time((end_time - st) / (step + 1) * (steps - step - 1))\n",
        "        print('\\r    Step %d / %d, remaining time: %s, loss: %.4f'\n",
        "                % (step + 1, steps, remaining_time, total_loss / total_num), end='')\n",
        "    train_data.on_epoch_end()\n",
        "    et = time.time()\n",
        "    time_cost = format_time(et - st)\n",
        "    print('\\r    Step %d / %d, time cost: %s, loss: %.4f' % (steps, steps, time_cost, total_loss / total_num))\n",
        "    valid_loss, f1_score = evaluate_fn(model, valid_data, loss_fn, output_size, test_historical)\n",
        "    torch.save(model.state_dict(), os.path.join(param_path, '%d.pt' % epoch))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# I have included the training results in the `results/` directory. I did not yet run the baseline model to make comparisons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n",
        "\n",
        "The paper was reproducible. For this, I was able to download data MIMIC-III and MIMIC-IV data from https://physionet.org/. And then for the rest, from data processing to model building to model training and metrics evaluation, I was able to use the original papers code from https://github.com/LuChang-CS/Chet/tree/master. After looking at the performance of the model against the test dataset, the results align with the those from the paper for the heart failure prediction task. The inital runs were based on the replicated Chet model with just 5 epoches instead of the 200 epoches specified in the paper because of the runtime limitations. \n",
        "It was really easy to replicate the whole process mentioned in the paper because they provided the source code. But it was a little difficult to get the data. I initally used the MIMIC-III demo dataset which ended up not giving any results as the dataset was too small. So it is hignly suggested to get access to the full dataset before reproducing the Chet model. and also the model takes a long time to run so it was difficult to run with 200 epoches so for this initial testing purposes I only ran 5 epoches but I will eventually run the full 200 and compare the data. \n",
        "As part of the next phase, I will start to I will continue to test this model on MIMIC-IV data as for now I only ran it on MIMIC-III dataset. Also I have not yet tested the diagnosis prediction task so I will test the model for this task with both the datasets. The other thing that is mentioned in the paper is the baseline model which I have not built and tested with, so I can not confirm if the Chet model outperfomes the baseline models like the paper claims."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "1. Lu, C., Han, T., & Ning, Y. (2022). Context-Aware Health Event Prediction via Transition Functions on Dynamic Disease Graphs. Proceedings of the AAAI Conference on Artificial Intelligence, 36(4), 4567-4574. https://doi.org/10.1609/aaai.v36i4.20380\n",
        "2. Johnson, Alistair, et al. \"MIMIC-IV\" (version 2.2). PhysioNet (2023), https://doi.org/10.13026/6mm1-ek67.\n",
        "3. Johnson, A., Pollard, T., & Mark, R. (2019). MIMIC-III Clinical Database Demo (version 1.4). PhysioNet. https://doi.org/10.13026/C2HM2Q.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
